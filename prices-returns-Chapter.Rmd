---
title: "Prices Periods Returns Chapter"
output:
  html_notebook: default
  html_document:
    df_print: paged
  pdf_document: default
---


```{r setup, include = FALSE}
knitr::opts_chunk$set(message=FALSE, warning=FALSE)
library(tidyverse)
library(tidyquant)
library(highcharter)
library(timetk)
library(tibbletime)
```

Returns. 

Where are we headed in this chapter. 

1) From a practical perspective, here's what we need to do. 
Import prices for each of our assets. Here we will be building a 5-asset portfolio, that is balanced and diversified, we believe. 

We will import daily prices for our 5 assets since January 1, 2005. 

Our prices will be imported daily, consist of open, close, adjusted, volume, high, low. first decision: time period, which group - adjusted. then, which period, keep daily, convert to monthly, weekly, quarterly. we'll go with monthly. Now we have monthly prices for a certain time period for 5 stocks. we need to convert those prices to returns - what kind? log returns as will be important when we simulate. 

2) from a workflow perspective, we have just plunged into the data science process, which in the tidyverse we think of as beginning with data import and then progressing to data wrangling - which means getting the 'raw' data into the format we want to use. Here, the 'raw' data are a time series of prices for our 5 stocks, taken from Yahoo! finance. Important things to think about: 

The source - in industry, you almost certainly won't be pulling from yahoo! or google, you will have an internal database or data lake that holds the time series data. For anyone wishing to reproduce or reuse or build upon your work, a crucial first step is being able to import or update your raw data. It's a simple but oft overlooked first step that needs to be made clear. 

which dates? Why these dates? Does it matter? I chose 2005 as the start date so that we could observe the credit crunch/financial crisis of 2008. I think that's interesting. Maybe my colleagues think it's cherry picking, maybe my clients think I need to go back to before the internet bubble. They are entitled to to their opinions, and I to mine. The important thing is to briefly explain our reasoning, and make it easy for someone to test his/her own permutations. If a colleague looks at our work and wants to test a start date that goes back to the internet bubble, we need to enable that.

This harkens back to the reproducible mindset. It seems our colleague is questioning the validity of our decision, and we can respond in one of two ways. I could be offended and defensive, and reflect that in my code by not making it easy to poke my data import. Or I could welcome this as a way to make my code and hypotheses stronger. If my organization has a culture that punishes or rewards, that flow into my coding decisions. 

The most successful organizations are those that, at a minimum, make this seemless and ideally make this process part of the expectation. An analogy might be to academic research where findings are expected to be peer-reviewed and scrutinized. It's not an insult or a challenge, rather it's part of the normal treatment of any finding.  In the realm of portfolio analysis, step 1 in this process is being able to import the raw price data.

Here, when we are using publicly available data, it's quite easy for someone to change the dates, or even the assets. They can simply edit "2005-01-01" to whatever date is desired or drop in different ticker symbols. 

In industry, it might not be so simple. Do our colleagues have access to the database? Maybe we are putting together a bespoke portfolio for a client that doesn't want other analysts to know his identity - can we sanitize this data or have a data lake that has anonymized our portfolios? 

There are a lot of issues here, and note that we have not got past our first 8 lines of code.  This is a good time to reemphasize that the code is often times not the most difficult part of data science in finance. Here, the data import code is simple and easily reproducible. But, by making it simple and reproducible, and encouraging this as the standard for the data import step in our team's analysis, we create the expectation that our work is going to be reproduced, reused and re-tested under different assumptions.  The code is simple; the implications are not.  

Beyond culture, there's an alpha-generating payoff to setting the tone of reproducibility in step 1.  As we noted in the introduction, finance isn't physics and a good strategy today will most likely be arbitraged away tomorrow.  Models and strategies need to be constantly scrutinized and updated, and not just by the original creator of the strategy.  The right data import workflow is the first crucial step to reproducibility, and reproducibility allows for models to be updated as market conditions change.

Once we have finished and explained step 1, data import, we immediately started to change the raw data. 
[
This is about mapping the Hadley data science paradigm onto the world of portfolio management.
]


## Choose Portfolio Building Block

For this chapter we will be working with a 5-asset portfolio consisting of the following.

    + SPY (S&P500 fund) weighted 25%
    + EFA (a non-US equities fund) weighted 25%
    + IJS (a small-cap value fund) weighted 20%
    + EEM (an emerging-mkts fund) weighted 20%
    + AGG (a bond fund) weighted 10%

I chose those 5 assets because they seem to offer a balanced blend of large, small, international, emerging and bond exposure. We will include several Shiny apps with each chapter and that will afford the opportunity to choose different assets and see different results.  For the code included in this book, we will not change or deviate from these 5 assets.

### Importing the data

On to step 1, wherein we import adjusted prices for the 5 ETFs to be used in our porftolio and save them to an `xts` object called `prices`.

We need a vector of ticker symbols that we will then pass to Yahoo! Finance via the `getSymbols` function from the `quantmod` package. This will return an object with the opening price, closing price, adjusted price, daily high, daily low and daily volume. We don't want to work with all of those, though. The adjusted price is what we need. 

To isolate the adjusted price, we use the `map` function from the `purrr` package and apply `Ad(get(.))` to the imported prices. This will 'get' the adjusted price from each of our individual price objects. We could stop here and have the right substance, but the format wouldn't be great as we would have a `list` of 5 adjusted prices. The `map` function returns a list by default. 

The `reduce(merge)` function will allow us to merge the lists into one object and coerce back to an `xts` structure.  Finally, we want intuitive column names and use `colnames<-` to rename the columns.  The `rename` function from `dplyr` will not work well here because the object structure is still `xts`.

```{r, message=FALSE, warning=FALSE}
# The symbols vector holds our tickers. 
symbols <- c("SPY","EFA", "IJS", "EEM","AGG")

# The prices object will hold our raw price data throughout this book.
prices <- 
  getSymbols(symbols, src = 'yahoo', from = "2005-01-01", 
             auto.assign = TRUE, warnings = FALSE) %>% 
  map(~Ad(get(.))) %>% 
  reduce(merge) %>%
  `colnames<-`(symbols)

```


### Convert Daily Prices to Monthly Returns


Next we want to turn those daily prices into monthly returns. This seems like a rather innocuous step in our work, but it involves two important decisions to be highlighted in the name of reproducibility. First, we are changing time periods from daily to monthly.  Not crucial, but we are transforming our data. We need to explain how that's happening.

More importantly, we will be transforming our data from its raw form, adjusted prices, to a calculated form, log returns.  

This is such a simple step that temptation is to include a few lines of code and move on to the analysis, which is the stuff our team gets paid to do. But, converting to log returns is our first major data process decision: why did we choose log returns instead of simple returns? It's a standard practice to use log returns but it's also a good chance to set the precedent amongst our team and within our workflow that we justify and explain decisions about our data.  If we have made the decision to work with log returns across our work, we should point to an R Notebook or a PDF that explains the decision and the brief substantive justification.  In this case, I know that simulating returns is in our future, and we will be assuming a normal distribution of returns. Thus, I choose to convert to log returns. In industry and when establishing a data science practice, an explanatory R Notebook can serve three purposes. First, for new team members, they will have a reference library that helps contextualize team-wide decisions.  Second, should anyone every ask, why have we chosen log returns as the standard? The team can point to the reference material, and invite theoretical disagreements should the questioner not agree with that material. Third, it sets the standard for a best practice: this team justifies decisions that affect our data and conclusions.  

For the purposes of this book, we will actually and visualize the difference between log returns and simple returns as part of our workflow. 

First, we will use the `to.monthly` function from the [quantmod](https://www.quantmod.com/) to turn daily prices to monthly prices. 

```{r, message=FALSE, warning=FALSE}

prices_monthly <- to.monthly(prices, indexAt = "last", OHLC = FALSE)
``` 



Now we'll call `Return.calculate(prices_monthly, method = "log")` to convert to returns and save as an object call `portfolio_component_returns`.

```{r, message=FALSE, warning=FALSE}
asset_returns_xts <- na.omit(Return.calculate(prices_monthly, method = "log"))

head(index(asset_returns_xts))
```

Take a quick look at the monthly returns above, to make sure things appear to be in order. Notice in particular the date of the first value. We imported prices starting "2005-01-01" yet our first monthly return is for "2005-02-28".  That is not necessarily good or bad, but it might matter if that first month's returns makes a difference in our analysis. 

We are not going to take that same raw data, which is the `prices` object we created upon data import and convert it to monthly returns using 2 alternative methods. We will make use of the `tidyquant`, `timetk` and `tibbletime` packages. 

Let's get to the code flow before explaining the motivation for doing seemingly redundant work. 

[OR explain the motivation right here]

First we will use just `tidyquant` and `timetk` to convert our object from an xts object of prices to a `tibble` of monthly returns. Once we convert to `tibble` the tidyverse is available for all sort of manipulations. 

In the piped workflow below, our first step is to use the `tk_tbl(preserve_index = TRUE, rename_index = "date")` function to convert from xts to tibble. The two arguments will convert the `xts` date index to a date column, and rename it "date". If we stopped here, we would have a new prices object in tibble format. 



```{r}
# Use the xts prices that were converted to monthly
# Then timetk
asset_returns_tk_byhand <- 
  prices_monthly %>% 
  tk_tbl(preserve_index = TRUE, rename_index = "date") %>%
  gather(asset, returns, -date) %>% 
  group_by(asset) %>%  
  mutate(returns = (log(returns) - log(lag(returns)))) %>%
  spread(asset, returns) %>% 
  select(date, symbols)

asset_returns_tq_builtin <- prices %>%
  tk_tbl(preserve_index = TRUE, rename_index = "date") %>%
  gather(asset, prices, -date) %>% 
  group_by(asset) %>%
  tq_transmute(mutate_fun = periodReturn, period = "monthly", type = "log") %>% 
  spread(asset, monthly.returns) %>% 
  # We want to make sure the columns are in the order that we think they are
  select(date, symbols)

```

Take a look at the new returns object and notice that it looks identical to the previous returns object. There is one big difference which is that it is a tibble object instead of an xts object. More importantly, this object is not in tidy format.  

The next code flow will accomplish exactly the same thing, but we will also introduce the `tibbletime` package and it's function `as_period`.  As the name implies, this function allows us to cast the prices time series from daily to monthly (or weekly or quarterly etc.), before calculating returns. Whereas above we went from daily to prices to monthly returns, here we go from daily prices to monthly prices to monthly returns.  

We don't have a substantive reason for doing that here, but it could prove useful for recasting date periods in financial data. 


```{r}

asset_returns_tbltime <- prices %>% 
  tk_tbl(preserve_index = TRUE, rename_index = "date") %>%
  tbl_time(index = "date") %>% 
  as_period("monthly", side = "end") %>%
  gather(asset, returns, -date) %>% 
  group_by(asset) %>% 
  tq_transmute(mutate_fun = periodReturn, period = "monthly", type = "log") %>% 
  spread(asset, monthly.returns) %>% 
  select(date, symbols)

```

Let's take a peek at our three monthly return objects.

```{r}
head(asset_returns_tbltime)
head(asset_returns_tqtbl)
head(asset_returns_xts)
```

Do we notice anything of interest? 

For some reason, only one of our objects, `asset_returns_tqtbl`, contains a value for the month of January. `asset_returns_tbltime_byhand` reports a January 2005 return of NA, and `asset_returns_xts` simply excludes the observation.  Does it matter? Well, probably yes.  It will affect all of our analysis for the remainder of this book and if we were constructing a true trading strategy or portfolio management workflow, real money could be affected if we use the object that includes or excludes the first observation.

Since our goal was to start at January 1, we will work with the most inclusive object and keep the January 2005 return as part of our data set.  
 
Let's recap what we've done thus far. We have imported raw price data for 5 assets, in a reprudicible and flexible way.  We have used three different methods for converting those daily prices to monthly, log returns.  From those three methods, we now have three objects: `asset_returns_xts` (an xts object), `asset_returns_tqtbl` (a tibble object created with tidyquant and timetk) and `asset_returns_tbltime` (a tibble object created with tidyquant, timetk and tibbletime packages). 

First, we can think of this as step 1 in a wholistic data science workflow, that begins with data import and transformation. To make this reproducible, step 1 needs to be so crystal clear that our colleagues will find it insultingly easy to follow our work. If we wish for our work to lay the ground work for several potential projects or test strategies, this first step needs to be clear and accessible.

Second, there's a high likelihood that we will encounter work from other team members who have their own methods for data import and transformation.  The more methods we can master or at least practice, the better prepared we will be to reuse or expand on our colleagues' work.

Third, data import and transofrmation is straightforward, but it also forces us to engage with our data in its rawest form, instead of skipping ahead to the model and the R squared.  To me, a data scientist can never spend too much time getting to know his/her data. Perhaps new insights will jump out, or an error will be found, or a new hypothesis.  Furthermore, when it comes time to defend or update our findings or conclusions, deep knowledge of the raw data is crucial. 

Finally, if importing and transforming data seems painfully dull, I completely understand. But, it's part of being a data scientist. If this process is unbearably dull, data science might not be the ideal career path. 
We could jump straight into the process of converting these assets into a portfolio, but it's good practice to have a quick peek at the individual charts before doing so. I find that once a portfolio is built, we're unlikely to back track to visualizing returns on an individual basis. Yet, those individual returns are the building blocks the raw material of our portfolio. Visualizing their returns adds another chance to get to know our raw data. 

### Visualize Individual Returns

Now, let's visualize those individual asset returns. We are going to use the `asset_returns_tqtbl` for our visualization and `ggplot` to create a histogram. 

Careful readers and those familiar with the tidyverse might already know that we have a problem - our data is *not* in tidy format.  Each of our assets has its own column, which makes it quite readable for us humans. But it is not tidy. To be tidy, each variable needs to have its own column. And here, the variable "asset", does not have a column. Instead, each asset has its own column! In tidyverse nomenclature, this data is in wide format instead of long format. 

```{r}
asset_returns_tqtbl_long <- 
  asset_returns_tqtbl %>%
  gather(asset, return, -date)
  
  
head(asset_returns_tqtbl_long)
head(asset_returns_tqtbl)
```

The best way to grasp the difference between long and wide data is to look at examples. The first data frame above is in long, tidy format. Each variable has its own column. This is how tidyverse packages like ggplot and dplyr want their date. The lower data frame is in wide format. I find this easier to digest as a human so I usually peak at my data in wide format. 



Now we can visualize all 5 assets in one chart: 


```{r}
# Some aesthetic housekeeping: make so all titles centered.
theme_update(plot.title = element_text(hjust = 0.5))

asset_returns_tqtbl_long %>% 
  ggplot(aes(x = return, colour = asset, fill = asset)) +
  stat_density(geom = "line", alpha = 1) +
  facet_wrap(~asset) +
  ggtitle("Monthly Returns Since 2005") +
  xlab("monthly returns") +
  ylab("distribution") 
```

```{r}
# Some aesthetic housekeeping: make so all titles centered.
theme_update(plot.title = element_text(hjust = 0.5))

asset_returns_tqtbl_long %>% 
  ggplot(aes(x = return, colour = asset, fill = asset)) +
  geom_histogram(alpha = 0.25, binwidth = .005) +
  facet_wrap(~asset) +
  ggtitle("Monthly Returns Since 2005") +
  xlab("monthly returns") +
  ylab("distribution") 
```

We can also combine these into one chart. Do we love the black font color? It's alright but I try to choose a color that doesn't appear very frequently: corn flower blue. It's a nice color but more importantly, it will help viewers to begin to associate your team's charts with a certain color scheme.

```{r}

# Some aesthetic housekeeping: make so all titles centered.
theme_update(plot.title = element_text(hjust = 0.5))

asset_returns_tqtbl_long %>% 
  ggplot(aes(x = return, colour = asset, fill = asset)) +
  stat_density(geom = "line", alpha = 1) +
  geom_histogram(alpha = 0.25, binwidth = .005) +
  facet_wrap(~asset) +
  ggtitle("Monthly Returns Since 2005") +
  xlab("monthly returns") +
  ylab("distribution") +
  theme(plot.title = element_text(colour = "cornflowerblue"),  
        strip.text.x = element_text(size = 8, colour = "cornflowerblue"), 
        strip.background = element_rect(colour = "cornflowerblue", fill = "white"), 
        axis.text.x = element_text(colour = "cornflowerblue"), 
        axis.text = element_text(colour = "cornflowerblue"), 
        axis.ticks.x = element_line(colour = "cornflowerblue"), 
        axis.text.y = element_text(colour = "cornflowerblue"), 
        axis.ticks.y = element_line(colour = "cornflowerblue"),
        axis.title = element_text(colour = "cornflowerblue"),
        legend.title = element_text(colour = "cornflowerblue"),
        legend.text = element_text(colour = "cornflowerblue")
        )
```

Now that we have visualized, do we notice anything about the distribution of returns? We will discuss this in the next chapter when we consider our portfolio and examine how combining these 5 assets leads to a new distribution of returns. 


### To the Portfolio Station


Going from a collection of individual returns to a portfolio, which is really a weighted collection of asset returns. Accordingly, the first thing we need to do is assign a weight to each portfolio. Recall that our vector of weights is `r symbols`. Let's create a weights vector that will allow us to assign a weight to each of our symbols. We are going for a balanced portfolio but we will still weight relatively little to AGG, the bond fund.


```{r, message = FALSE}
w <- c(0.25, 0.25, 0.20, 0.20, 0.10)
```

Before we use the weights in our calculations, a quick sanity check in the next code chunk is a good idea. This might not be necessary with 5 assets as we have today, but good practice because if we had 50 assets it could save us a lot of grief to catch a mistake early.

```{r Weights Sanity Check}
# Make sure the weights line up with assets.
# This is why we made sure all the return objects were in the same order as the
# `symbols` vector.
asset_weights_sanity_check <- tibble(w, symbols)
asset_weights_sanity_check

```

Put in the portfolio code; by hand returns; built in function for contributions from perfan and tidyquant? 



